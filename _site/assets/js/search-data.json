{"0": {
    "doc": "Empirical-Statistical Downscaling",
    "title": "Introduction",
    "content": ". | The need for climate information | Motivation | History | The new tool: ‘esd’ | . ",
    "url": "http://localhost:4000/about/#introduction",
    "relUrl": "/about/#introduction"
  },"1": {
    "doc": "Empirical-Statistical Downscaling",
    "title": "The need for climate information",
    "content": "A call for climate services and other initiatives and programmes to provide climate informa- tion to stake holders and the general public has been made over the recent decades (e.g. the WMO global framework for climate services (GFCS), the CORDEX program, and the Joint Programming Initiative (JPI-Climate)). The urgency of their mission has been emphasised by recent extreme weather events and the publication of a number of climate research reports (e.g. Hov et al. (2013); Field et al. (2012); Stocker et al. (2013)). The development of efficient and versatile tools for climate analysis are essential to this effort. For instance, there is always a need for extracting relevant climate information, reading data, and testing and visualising the results. The present ‘esd’ tool has been developed to meet these requirements. The ‘esd’ tool is being used in different projects such as FP7 SPECS and CLIPC1, COST- VALUE2, INDICE3, CixPAG4, MIST-II 5, CLIMATRANS6, and EU-CIRCLE 7 to establish networks and standards for assessment and validation of model results needed for climate ser- vices. ",
    "url": "http://localhost:4000/about/#the-need-for-climate-information",
    "relUrl": "/about/#the-need-for-climate-information"
  },"2": {
    "doc": "Empirical-Statistical Downscaling",
    "title": "Motivation",
    "content": "Open, efficient, flexible, and intuitive tools making use of state-of-the-art statistical know-how have long been needed for the purpose of climate analysis. Often the lack of resources (man power) dedicated to such work limits the possibilities of offering tailor-made user-relevant infor- mation. There is a wide range of requirements, from climate data handling and analysis, and validation/diagnostics to bias adjustment of model results and downscaling (including predic- tion and projection). Even if the main purpose may be just to carry out empirical-statistical downscaling (ESD; Benestad et al. (2008)), many other tasks are necessary to prepare the anal- ysis and understand the results. The need to analyse multi-model ensembles rather than single model (Benestad , 2011) also makes additional demands on the tools. In other words, ‘esd’ is a tools for ’Big data’ climate analysis. 1CLIPC will provide access to climate information of direct relevance to a wide variety of users, from scientists to policy makers and private sector decision makers. 2The COST Action VALUE (2012-2015) aims at providing a European network to develop downscaling meth- ods, validate them, and improve the collaboration between the research communities and stakeholders. 3INDICE is a collaboration between Norway and India that studies the hydrological consequences of climate change on the Hindu-Kush region in Himalaya 4CiXPAG project aims at investigating the complex interactions between climate extremes, air pollution and agricultural ecosystems. This project aims at providing reliable projections to be used by STATKRAFT as a leading company in hydro-power internationally and Europe’s largest generator of renewable energy. 6CLIMATRANS project aims at investigating the complex interactions between climate extremes, air pollution and transportation in Indian mega-cities. 7EU-CIRCLE project aims at developing a Climate Infrastructure Resilience Platform (CIRP) as an end-to-end modelling environment. An important aspect of any climate analysis tool is data input-output (I/O) and handling data. Although model results are meant to be provided in a standard format (e.g. the Coupled Model Inter-comparison Project8 - CMIP - following the ‘Climate and Forecast’ (CF) convention), experience shows that there are often differences that can cause problems. For instance, the CMIP5 ensemble of models involves 4 different types of calendars. Data files are often organised in accordance with some kind of Common Information Model9 (CIM) or a Data Reference Syntax10 (DRS), but there is also a benefit in standard structures in the working computer memory when using programming environments such as R. A well-defined and standardised data structure makes it possible to create a generic climate analysis tool. Searching for available and complete observational data sets is always a difficult and time consuming task without prior knowledge about the quality of the retrieved data sets. In the context of empirical-statistical downscaling, observations are needed both for validation of model results and for calibrating models. Furthermore, there may be a need to aggregate annual mean values rather than daily or monthly, and to extract subsets, for instance a given calendar month, a limited interval, or a smaller spatial region. The purpose of the ‘esd’ package is to offer an R-package that covers as many as possible of these requirements, from acquiring data and converting to a standardised format, to performing various statistical analyses, including statistical downscaling, all in a simple and user-friendly way. ",
    "url": "http://localhost:4000/about/#motivation",
    "relUrl": "/about/#motivation"
  },"3": {
    "doc": "Empirical-Statistical Downscaling",
    "title": "History",
    "content": "Most of the previous work on ESD carried out at MET Norway has been based on the R- package ‘clim.pact’ (Benestad et al., 2008) from the early 2000s. This tool had evolved into a large set of methods and functions over time, but without a ‘strict discipline’. Hence, the functions would use a range of different set-ups and lacked a streamlined finish. Therefore, it was not very user friendly. The idea is that any climate data analysis should be quick, easy, and efficient, with a bare minimum effort going into coding and reformatting of data sets. It should be intuitive and easy to remember how to perform the analysis. This way, more of the time can be devoted to do the analysis itself and writing reports, rather than rewriting code, debugging, testing, and verifying the process. While much of the methods needed in climate analysis were included in ‘clim.pact’, it was evident that the most efficient solution was to create a new R-package from scratch with a more purposeful and well-defined data structure. Much of the methods, however, are based on functions in ‘clim.pact’, although the benefit of experience from developing that package has been utilised from the start of the creation of ‘esd’. 8http://cmip-pcmdi.llnl.gov/ 9https://www.earthsystemcog.org/projects/downscalingmetadata/ 10http://cmip-pcmdi.llnl.gov/cmip5/output_req.html . Since it is creation, the ‘clim.pact’ package has been widely used by the climate community for research purposes and has been a valuable asset for assessment studies (Winkler et al., 2011). For instance, Spak et al. (2007) compared statistical and dynamical downscaling of surface temperature in North America using results obtained from the ‘clim.pact’ tool, and Girma et al. (2010) used the ‘clim.pact’ tool to downscale rainfall in the upper Blue Nile basin for hydrological modelling. The R-package was used by Sch¨ oner and Cardoso (2005) to downscale air temperature mean and precipitation sums from regional model as well as directly from ERA-40 fields, and Tyler et al. (2007) used it to produce temperature scenarios by downscaling output from 17 different global climate. However, Estrada et al. (2013) pointed out that ‘clim.pact’ and similar tools do not provide the necessary tests to ensure the statistical adequacy of the model and therefore misleading results could be expected. We will address some of the issues that Estrada et al. (2013) highlighted later on. ",
    "url": "http://localhost:4000/about/#history",
    "relUrl": "/about/#history"
  },"4": {
    "doc": "Empirical-Statistical Downscaling",
    "title": "The new tool: ‘esd’",
    "content": "The new tool ‘esd’, like its predecessor ‘clim.pact’, is based on a ‘plug-and-play Lego principle’, where data are seen as an object that is used as input and new results are returned as output following a standard data structure. This means that different functionalities can be combined where the output from one process is used as input to the next. In ‘esd’ the data structure has changed from that in ‘clim.pact’, and uses more attributes and less list objects. Furthermore, the data objects build on the time series object ‘zoo’, which takes care of much of the chronology. A consequence of using ‘zoo’ as a basis also is that it implies adopting S3-methods, which also makes the tool more flexible and user-friendly. Through the S3- methods, new plotting methods have been made available for station and field objects, empirical orthogonal functions (EOFs), and other types. The use of S3-methods implies the definition of new classes describing the data types. It can also make the tool work more seamlessly with other R-packages and other structures if appropriate conversion tools are made. The development of the ‘esd’ software fits in with the trend of the R-language’s increasing role in the climate change debate (Pocernich, 2003) and as an open science platform (Pebesma et al., 2012). Furthermore, both R and the ‘esd’ R-package are valuable tools for linking high education and research (IBARRA-BERASTEGI et al.). The ‘esd’-package has already been used as material for capacity building in connection with the bilateral INDICE project between Norway and India (http://www.nve.no/en/Projects/INDICE/), and the CHASE-PL project between Norway and Poland (http://www.isrl.poznan.pl/chase/index.php/project/). The wide range of functionalities included in the ‘esd’ tool makes it suitable for process- ing results from global and regional models. The tool also includes methods for plotting and generating various info-graphics: time series, maps, and visualisation of complex information. The data processing aspects include re-gridding, sub-setting, transformations, computing em- pirical orthogonal functions (EOFs) and principal component analysis (PCA) for stations and gridded data on (ir)regular grids, regression, empirical-statistical downscaling, canonical correla- tion analysis (CCA) and multi-variate regression (MVR). The tool also offers several diagnostic methods for quality assessment of the data and downscaled results. The ‘esd’ methods can be tailored to meet with specific user needs and requirements. As the library was built for the R computing environment (R Core Team, 2014), it inherits from the large number of R built-in functions and procedures. It also includes an additional set of predefined objects and classes, sample and structured data and meta-data, and uses the S3-methods to ensure that information is appropriately maintained. Finally, the ‘esd’ library has been built with the emphasis of traceability, compatibility, and transparency of the data, methods, procedures, and results, and is made freely available for use by the climate community and can be installed from the MET Norway Github account (https://github.com/metno/esd) or Figshare (http://dx.doi.org/10.6084/m9.figshare.1160493). The remainder of this report describes technical issues related to the ‘esd’ tool. A listing of the syntax and examples are also included in this report. ",
    "url": "http://localhost:4000/about/#the-new-tool-esd",
    "relUrl": "/about/#the-new-tool-esd"
  },"5": {
    "doc": "Empirical-Statistical Downscaling",
    "title": "Empirical-Statistical Downscaling",
    "content": " ",
    "url": "http://localhost:4000/about/",
    "relUrl": "/about/"
  },"6": {
    "doc": "examples",
    "title": "Examples",
    "content": ". | Example 2.1 on how to select weather stations and retrieve recorded values | Example 2.2 on the different esd objects and clasees | Example 2.4 on how to load some data sets and display the summary of statistics | Example 2.5 on how to load a weather station object and display the location on a map | Example 2.6 on how to read a field object and plot the results | Example 2.7 on how to retrieve wind data sets and visualise the results | Example 2.8. test | Example 2.9 on how to process the data and visualise the trend of the time series | . ",
    "url": "http://localhost:4000/examples/#examples",
    "relUrl": "/examples/#examples"
  },"7": {
    "doc": "examples",
    "title": "Example 2.1 on how to select weather stations and retrieve recorded values",
    "content": "# Select a station across India recording daily maximum temperature # from the global historical climate network-daily ss &lt;- select.station(cntr=’India’,param=’tmax’,src=’ghcnd’) ss.new &lt;- subset(ss,subset=!duplicated(ss$location)) map(ss.new,cex=1.2,col=\"red\",bg=\"pink\",add.text=TRUE) ss &lt;- select.station(stid=’IN022021900’,cntr=’india’,param=’tmax’) y &lt;- station(ss) #&gt;[1] \"Retrieving data ...\" #&gt;[1] \"1 TMAX IN022021900 NEW DELHI/S INDIA GHCND\" # Display the name of the location loc(y) # Highlights the location on the map points(lon(y),lat(y),pch=19,col=\"blue\",cex=2) # aggregate daily values to annual values ya &lt;- annual(y,FUN=’mean’,nmin=100) # Subset for the period 1970 to 2012 ya &lt;- subset(ya,it=c(1970,2012)) # plot the time series including the error bar plot(ya,ylim=c(29.5,32.5)) # Add the linear trend as lines(trend(ya),col=\"red\",lwd=2) . Figure 3: Map showing available stations across India from the GHCN-D dataset and b) a plot of the annual maximum temperature recorded at New Delhi weather station (blue point) including linear trend line. ",
    "url": "http://localhost:4000/examples/#example-21-on-how-to-select-weather-stations-and-retrieve-recorded-values",
    "relUrl": "/examples/#example-21-on-how-to-select-weather-stations-and-retrieve-recorded-values"
  },"8": {
    "doc": "examples",
    "title": "Example 2.2 on the different esd objects and clasees",
    "content": "# Example of monthly station data: &gt; data(Oslo) &gt; class(Oslo) [1] \"station\" \"month\" \"zoo\" # Example of daily station data: &gt; data(ferder) &gt; class(ferder) [1] \"station\" \"day\" \"zoo\" # Example of annual station data: &gt; class(annual(ferder)) [1] \"station\" \"annual\" \"zoo\" # Example of a field object &gt; t2m &lt;- t2m.NCEP(lon=c(-30,30),lat=c(40,70)) &gt; class(t2m) [1] \"field\" \"month\" \"zoo\" &gt; class(EOF(t2m)) [1] \"eof\" \"field\" \"month\" \"zoo\" Example 2.3. # Load the data for Ferder weather station data(ferder) # Display the structure of the data as &gt; str(ferder) ’zoo’ series from 1900-01-01 to 2013-12-04 Data: atomic [1:41611] 2.1 -1.8 -0.9 -3 -7.2 -6.5 -2.6 -2.4 -1.6 -0.3 ... - attr(*, \"location\")= chr \"Ferder lighthouse\" - attr(*, \"station_id\")= num 27500 - attr(*, \"wmo_id\")= num 1482 - attr(*, \"longitude\")= num 10.5 - attr(*, \"latitude\")= num 59 - attr(*, \"altitude\")= num 6 - attr(*, \"variable\")= chr \"t2m\" - attr(*, \"unit\")= chr \"deg C\" - attr(*, \"country\")= chr \"Norway\" - attr(*, \"source\")= chr \"MET Norway eklima\" - attr(*, \"long name\")= chr \"daily mean temperature\" - attr(*, \"URL\")= chr \"http://eKlima.met.no\" - attr(*, \"calendar\")= chr \"gregorian\" - attr(*, \"quality\")= logi NA - attr(*, \"aspect\")= chr \"original\" - attr(*, \"type\")= chr \"observation\" - attr(*, \"reference\")= chr \"MET Norway climate archive\" - attr(*, \"info\")= logi NA - attr(*, \"method\")= chr \"mean estimated from thermometer measurements\" - attr(*, \"history\")=List of 3 ..$ call : chr \"1 0\" ..$ timestamp : chr \"Fri Dec 13 15:51:49 2013\" ..$ sessioninfo:List of 3 ...$ R.version : chr \"R version 3.0.2 (2013-09-25)\" ...$ esd.version: chr \"esd_0.2-1\" ...$ platform : chr \"x86_64-pc-linux-gnu (64-bit)\" Index: Date[1:41611], format: \"1900-01-01\" \"1900-01-02\" \"1900-01-03\" \"1900-01-04\" ... ",
    "url": "http://localhost:4000/examples/#example-22-on-the-different-esd-objects-and-clasees",
    "relUrl": "/examples/#example-22-on-the-different-esd-objects-and-clasees"
  },"9": {
    "doc": "examples",
    "title": "Example 2.4 on how to load some data sets and display the summary of statistics",
    "content": "## Load data for Ferder &gt; data(ferder) ## Display the summary of statistics &gt; summary(ferder) . ",
    "url": "http://localhost:4000/examples/#example-24-on-how-to-load-some-data-sets-and-display-the-summary-of-statistics",
    "relUrl": "/examples/#example-24-on-how-to-load-some-data-sets-and-display-the-summary-of-statistics"
  },"10": {
    "doc": "examples",
    "title": "Example 2.5 on how to load a weather station object and display the location on a map",
    "content": "# Load bjornholt data set data(bjornholt) # plot the time series plot(bjornholt) # map the location map(bjornholt) . Figure 4: Example of a station (a) and a map plot (b) for a single station ‘Bjornholt’. ",
    "url": "http://localhost:4000/examples/#example-25-on-how-to-load-a-weather-station-object-and-display-the-location-on-a-map",
    "relUrl": "/examples/#example-25-on-how-to-load-a-weather-station-object-and-display-the-location-on-a-map"
  },"11": {
    "doc": "examples",
    "title": "Example 2.6 on how to read a field object and plot the results",
    "content": "#Get NCEP 2m air temperature for the selected spatial window defined by lon and lat t2m &lt;- t2m.NCEP(lon=c(-30,30),lat=c(40,70)) # Computes the EOFs X &lt;- EOF(t2m) # Plot the result plot(X) . Figure 5: An example of a plot results for an EOF object. ",
    "url": "http://localhost:4000/examples/#example-26-on-how-to-read-a-field-object-and-plot-the-results",
    "relUrl": "/examples/#example-26-on-how-to-read-a-field-object-and-plot-the-results"
  },"12": {
    "doc": "examples",
    "title": "Example 2.7 on how to retrieve wind data sets and visualise the results",
    "content": "# Load 10m zonal and meridional wind components u10 &lt;- retrieve(’data/ERAINT/eraint_elnino.nc’,param=’u10’) v10 &lt;- retrieve(’data/ERAINT/eraint_elnino.nc’,param=’v10’) # Map the data map(u10,colorbar=FALSE) # Display the vectors vec(u10,v10,new=FALSE,a=2,length=0.05) . ",
    "url": "http://localhost:4000/examples/#example-27-on-how-to-retrieve-wind-data-sets-and-visualise-the-results",
    "relUrl": "/examples/#example-27-on-how-to-retrieve-wind-data-sets-and-visualise-the-results"
  },"13": {
    "doc": "examples",
    "title": "Example 2.8. test",
    "content": "# Get 2m temperature data for Oslo # (works within MET Norway firewall only) x &lt;- station(stid=18700,param=’t2m’,src=’metnod’) x &lt;- subset(x,it=c(1837,2014)) ## Or gets data from ECA&amp;D as ## x &lt;- station(loc=\"Oslo Blindern\",param=’t2m’,src=’ecad’) ## x &lt;- subset(x,is=duplicated(loc(x))) # to remove duplicated stations # Cumulative average cumugram(x) # Seasonal wheel wheel(x) # seasonal variations of year-to-year variance climvar(x) # daily seasonal cycle for all years diagram(x) . Figure 7: Examples of ‘cumugram’, ‘wheel’, ‘climvar’, and ‘diagram’ plots. The ‘cumugram’ shows how the mean value of some variable has evolved from the start of the year and compares this curve to previous years. The graphics produced by ‘wheel’, on the other hand, emphasises how the seasonal variations affect the variable, e.g. whether some extremes tend to be associated with a specific season. Panel c shows results produced by ‘climvar’ shows the year-to-year statistics for a variable, e.g. the standard deviation of the temperature on February 1st. The ‘diagram’ method can be used in different context, and for a ‘station’ object, it produces graphics that compare the day-to-day values with those of previous years. ",
    "url": "http://localhost:4000/examples/#example-28-test",
    "relUrl": "/examples/#example-28-test"
  },"14": {
    "doc": "examples",
    "title": "Example 2.9 on how to process the data and visualise the trend of the time series",
    "content": "# Get 2m temperature data for Ferder and calculate annual mean data(ferder) x &lt;- annual(ferder) # Plot the time series and add a trend line plot(x,ylim=c(4,11)) lines(trend(x),col=\"red\",lwd=2) # Visualise trends for various periods 40 years or longer (minlen) # and mark trends that are significant at the 1% level (pmax) vis.trends(x,minlen=40,pmax=0.01) t2m trend (deg C/decade) . Figure 8: Visualising the trend in the temperature time series from Ferder for the full period (left panel) and simultaneously for different periods of various length (right panel). The trend analysis indicates a significant increase in temperature at Ferder since the 1900s, but also shows that the strength and significance of the trend is sensitive to the period considered because the increase is not monotone. ",
    "url": "http://localhost:4000/examples/#example-29-on-how-to-process-the-data-and-visualise-the-trend-of-the-time-series",
    "relUrl": "/examples/#example-29-on-how-to-process-the-data-and-visualise-the-trend-of-the-time-series"
  },"15": {
    "doc": "examples",
    "title": "examples",
    "content": " ",
    "url": "http://localhost:4000/examples/",
    "relUrl": "/examples/"
  },"16": {
    "doc": "Functionalities",
    "title": "Functionalities",
    "content": "A short and single line in R produces a complex figure with various information: the generic ‘plot’ yields a plot as seen in the left panel of Figure 4 or can make a graphical presentation of downscaled results that both shows the numbers as well as their quality. How is that possible? The trick is to define different data object types, known as ‘classes’ in R and define a specific data reference syntax (DRS) or common information model (CIM) that includes the meta-data in the working computer memory as well as in files stored on discs. This is all done automatically on-the-fly behind the scene, so that the user does not have to worry about these matters (it is of course possible to change the meta-data to e.g. correct for potential errors). The command library(‘esd’) must be given at any new open R session. The R-package can be installed directly from Github (https://github.com/metno/esd) or Figshare for Mac/Linux11 and Windows12) . | Retrieving data: I/O . | Function retrieve() | Function station() | Quick search - Function select.station() | . | Data structures . | Example 2.2 shows some of the classes used in ‘esd’. | . | Function summary() | Data visualisation . | Function plot() | Function map() | Function vec() | Info-graphics | . | . ",
    "url": "http://localhost:4000/functions/",
    "relUrl": "/functions/"
  },"17": {
    "doc": "Functionalities",
    "title": "Retrieving data: I/O",
    "content": "There are different types of data that can be handled by the ‘esd’ tool: station data, gridded data, and (storm) trajectories. Station data contain meteorological observations recorded at weather (or hydrological) stations, while gridded data can comprise various analyses (e.g. E-OBS gridded version of the European and Climate Assessment data set), reanalyses (e.g. NCEP, ERA, …) or global/regional climate model results (e.g. CMIP3/5 experiment). Trajectories are mainly used for analysis of storm tracks (e.g. IMILAST13). There are two main methods for retrieving data in the ‘esd’ tool: ‘station’ and ‘retrieve’. It is also possible to read data using the R-built in functions and convert it to esd data format. This requires more effort from the user using the ‘esd’ pre-defined functions as.station, as.field), and as.trajectory. The package comes with a set of sample data mainly included for demonstration purposes, testing, or troubleshooting. For gridded data, these are filtered data (mainly for reducing the size of the data objects) and should not be used as predictors in the actual analysis. For instance, air temperature reanalysis data are stored as a set of 20 empirical orthogonal functions (EOFs) with global coverage, which are then transformed to a field object upon a retrieval. However, the sample station data can be used without restrictions and corresponds to observed values at a specific location (e.g. data(Oslo) or nacd=station(src='nacd')). 11http://figshare.com/articles/esd_for_Mac_amp_Linux/1160493 12http://figshare.com/articles/esd_for_windows/1160494 13http://www.proclim.ch/imilast/index.html . Function retrieve() . The ‘retrieve’ is designed to read data from NetCDF files following the standard Climate and Forecast (‘CF’) conventions and ordered on a longitude-latitude grid. The latter could be regular or irregular grid (e.g. the output of the Weather Research and Forecasting model (WRF) or any outputs on a rotated grid from RCMs). The function ‘retrieve’ also performs quick checks of the data itself to verify that the meta data and data are consistent. For instance, in case the frequency of the data is missing, retrieve() will try to detect the frequency automatically form the data itself. The function retrieve returns two types of objects depending on the type of the spatial grid. For instance, data stored on a regular grid is returned as field objects including attributes containing meta data, and data stored on irregular grid - such as rotated longitude-latitude grids - are returned as a station objects. The function retrieve() has also been adapted to read global climate data from the CMIP3/5 experiments, most of the global reanalysis such as those provided by the European Centre for Medium-Range Weather Forecasts (ECMWF) known as ERA-40 and ERA-INTERIM, the National Center for Environmental Prediction (NOAA) known as NCEP/NCAR reanalysis, the Japan Meteorological Agency (Japanese Reanalysis JRA-25,55), and the NASA GSFC Global Modeling and Assimilation Office (GMAO) known as MERRA reanalysis. A full overview of all available reanalysis can be found at http://reanalysis.org/atmosphere/overview-current-reanalyses. As for now, retrieve(), does not display a list of missing attributes that are mandatory for further post-processing of the data. The user must add the missing attributes manually. The strength of retrieve() is that it can read and return formatted objects with common attributes for post-processing, visualising and making outputs comparable (e.g. re-gridding the field objects into the same grid resolution). Basically, all reanalysis, general circulation models (GCMs), and regional climate models (RCMs) can be read using the ‘esd’ tool and further combined into one object, or analysed, albeit with some limitations due to the size of the returned object. Function station() . The package ‘esd’ includes the function ‘station’ for obtaining historical climate data by querying various web portals, for instance, the MET Norway archive (KDVH) provided by the Norwegian Meteorological Institute14. Data from MET climate web service ‘eKlima’ needs to be adapted manually, the Global Historical Climate Network (GHCN, (Peterson and Vose,1997)) provided by the American National Climatic Data Center15, and the European Climate Assessment and Dataset (ECA&amp;D, (Klein Tank et al., 2002)) made available by the Royal Netherlands Meteorological Institute (KNMI) (http://eca.knmi.nl/). Some of the data that is 14http://eklima.met.no; however, this function only works within the firewall included in the package has been pre-formatted within the ‘clim.pact’ package and adapted to meet the new ‘esd’ data format requirements. 15http://www1.ncdc.noaa.gov/pub/ . SOURCE(S) : ECAD/GHCNM/METNOD/METNOM/NACD/NORDKLIM T2M/ 14632 2014 / 1773 ESD package − map.station() − MET Norway 2014 (www.met.no) Figure 1: Map of available weather stations recording temperature that are included in the meta-data of the ‘esd’ package. Quick search - Function select.station() . The sample data includes also a meta-data object (stationmeta) that can be loaded directly and contains meta data such as name of the location (loc) and its standard ientification number (stid), geographical coordinates such as longitude (lon), latitude (lat), and altitude (alt), country name (country), parameter name (param) of recorded weather variables (e.g. temperature and precipitation), data source (src) or provider for thousands of stations all over the world (Figures 1 and 2). These meta-data have been imported from existing meta data from the different data source providers. It has to be noted also that most of the available stations are managed by the World Meteorological Organisation (WMO) and made available by the American National Climate Data Centre (NCDC) for scientific research only. The meta data has been merged from the different sources mentioned earlier. Also, other additional data sources can be easily included into the package. Figure 2: Map of available weather stations recording precipitation that are included in the meta-data of the ‘esd’ package. There are two ways of obtaining station data using station() method. The first option, which we recommend, is to select a subset of stations from the meta data using the function select.station() from a set of criteria. Among these criteria, the minimum length of the recorded data (‘nmin’) can be specified to get, for instance, a subset of stations recording for at least a minimum number of (e.g. 100) years of data (e.g. select.station(nmin=100)). Thus, a subsample of the meta data is returned and can be checked for duplications of stations from the various sources. Afterwards, users can start downloading the data for the selected stations. It has to be mentioned that the downloading process for the GHCN and ECA&amp;D is different as the data is stored differently. For the GHCN data sets, each station is stored separately and a direct retrieval can be made. We highly recommend that users perform a prior selection of stations for the GHCN data sets before starting the downloading process. For the ECA&amp;D data sets on the other hand, all stations are downloaded and stored locally as zip files at first call, after which data is extracted for the selection of stations. Other data sets, such as the NACD (Frich et al., 1996), NARP (Førland ), and Nordklim (Tuomenvirta et al., 2001) are stored entirely in ‘esd’ (only holds monthly values of a limited set). The ‘station’ method can retrieve station data from a range of different sources, many over the web (GHCN and ECA&amp;D). Example 2.1 demonstrates how to select, retrieve and plot temperature observations from a single station. The ‘esd’ tool holds meta-data for various data sources, which makes it quick and easy to search for weather data recorded at stations according to the parameter, the geographical location (region, country, and coordinates (single or a range of longitude and latitude values) and altitude, and time interval. ",
    "url": "http://localhost:4000/functions/#retrieving-data-io",
    "relUrl": "/functions/#retrieving-data-io"
  },"18": {
    "doc": "Functionalities",
    "title": "Data structures",
    "content": "Most data objects handled by ‘esd’ are typically time series, and hence based on the zoo class. The zoo class extends the tseries class from regular to irregular time series. In the ‘esd’ tool, additional categories or classes have been defined dealing with temporal resolution distinguishing daily (‘day’), monthly (month), seasonal (season), and annual (annual) data (as temporal classes) and spatial resolution distinguishing location (station) and field (field) classes. Data on sub-daily scales may be represented in the ‘day’ class but the ‘esd’ tool has not been tested yet for this time scale. The way R handles different classes and objects can sometimes be confusing. The first element in a list of different classes is used to identify the appropriate method to be used for that specific object. For instance, if a data object belongs to the classes (‘field’,‘zoo’), then, appropriate methods for ‘field’ objects are used rather than those for the ’zoo’ objects. Different types of data objects are processed and handled differently, and the way ‘esd’ keeps track of the different types is through the so-called ‘S3-method’ and classes. The ‘esd’ tool follows the same R programming functionalities and uses the first element of the class of an object to define the appropriate method to be used. The built-in R functions class and inherits are used to check whether an object inherits from any of the classes specified in the ‘esd’ package. Example 2.2 shows some of the classes used in ‘esd’. The different data objects come bundled with relevant meta-data, stored as data attributes. These are shown using the str*ucture function, as displayed in Example 2.3 for a station object. The various attributes have different functions, e.g. for handling the data, traceability, identification, and visualisation. The idea is that they are based on a standard terminology for which the terms are commonly agreed on and follow standard definitions. A common core set of attributes will make it easier to share data and methods. The attribute history contains the call(s) (call), time stamp(s), and session info that have been used to create and process the data itself. ",
    "url": "http://localhost:4000/functions/#data-structures",
    "relUrl": "/functions/#data-structures"
  },"19": {
    "doc": "Functionalities",
    "title": "Function summary()",
    "content": "The S3 method ‘summary’ has been extended to the classes defined in ‘esd’ in order to provide more tailor-made information. Example 2.4 shows summary statistics for each calendar month of a daily station object. Table 1: Data objects in ‘esd’ are determined by a set of classes, listed in this table. This may be extended in the future to include radiosonde and radar data objects. ‘station’ Class defining station objects. Can be single or multiple stations with daily, monthly, seasonal or annual temporal resolution. ‘spell’ Looks similar to the station class, but the events are irregularly spaced and contains both duration of wet/hot as dry/cold spells. The distinction also enables ‘esd’ to apply different plotting and analysis methods than those for regular stations. ‘field’ Currently represents time series of 2D variables, but may in principle contain any number of spatial dimensions. ‘eof’ Class defining an EOF describing the spatial patterns (EOFs), the temporal variations (PCs), and the eigenvalues. ‘pca’ Class defining a PCA is similar to the eof class, but allows for irregular grid of stations. ‘cca’ Class that defines the results of a CCA, containing a pair of patterns and the canonical correlations. ‘ds’ Class for DS results. ‘dsensemble’ Class for downscaled ensembles. ‘diagnose’ Class for diagnostic results. ‘trajectory’ Class for trajectories. ‘xval’ Class for cross-validation. ‘xsection’ Class for cross-sections (Hovmuller diagrams). ‘mvr’ Class for multivariate regression (MVR) objects, which hold the matrices that maps one data set onto the data space of another. ",
    "url": "http://localhost:4000/functions/#function-summary",
    "relUrl": "/functions/#function-summary"
  },"20": {
    "doc": "Functionalities",
    "title": "Data visualisation",
    "content": "The main data visualisation is provided through plot and map methods, but there are other more specific methods for producing additional graphs. Function plot() . The ‘plot’ method in ‘esd’ extends the S3 plot methods from package ‘graphics’ to new ‘esd’ classes (Table 1). ‘plot(x)’ and ‘plot.station(x)’ are equivalent if ‘x’ is an object of class ‘station’. Various plotting outputs are generated depending on the class of the objects used as inputs. For instance, the ‘plot.station(x)’ function produces a figure based on the default ‘graphics’ and ‘zoo’ plots but adapted for the station object (Figure 4a). For some classes, plot can produce several or a combination of plots giving more description of the output. The argument ’plot.type’ is also used to distinguish between single or multiple plots in one window. The ‘plot’ function also inherits all graphical parameters from ‘par’ with additional parameters used by ‘esd’. An example of the function ‘plot’ applied to a station object is shown in Example 2.5 and Figure 4a. Although the plot itself gets more complicated for EOFs, the syntax remains as simple as for the station object (Example 2.6, Figure 5). Function map() . The function ‘map’ is also an S3 built method and used to produce a map of geographical and geophysical data points and gridded data. ‘map’ can be seen as a spatial plotting method, while plot is mainly used for time series. Unlike ‘plot’, ‘map’ is proper to ‘esd’ and is an S3 method and do not extend the existing ‘map’ function from package ‘maps’ (http://CRAN.R-project.org/package=maps), which too works for the different ‘esd’ objects. When applied to one single station, map plots its location (Example 2.5, Figure 4b). Function vec() . The function vec plots vectors of a 2D flow field (Example 2.7, Figure 6). Figure 6: The output of the example with ‘vec’. In this example, there are different plotting windows for the vectors and the underlying map, but this can be adjusted in the arguments of ‘vec’. Info-graphics . One of the purposes of ‘esd’ is to easily produce visualisation and graphics to bring out new aspects of the information embedded in the data. The development of the info-graphics in this tool has also been inspired by Spiegelhalter et al. (2011), in order to distill the essence of the analysis. The information stored in climate data can be extracted in various ways, with emphasis on different aspects, as can be seen in Example 2.8 and Figure 7. A cumugram() is shown, displaying the cumulative average value of some variable starting from the first day of the year. The results for the Oslo temperature in this example shows that 2014 has been the warmest year on record since the summer. A ‘rainbow structure’ is consistent with a gradual increase in the temperature. The second diagram, ‘wheel’, emphasises the time of the year when the most extreme events have taken place, and ‘climvar’ to the right shows how the year-to-year variance varies with season with a minimum in late summer. The ‘diagram’ method can also be used to view the data by comparing day-by-day values of the present temperature with those of the previous years. The figure shows that there have been some exceptionally mild autumn temperatures in 2014. Other functions for making info-graphics include vis.*, which make alternative graphics output displaying different information. Trends can be estimated by linear regression using the simple ‘trend’ function. An alternative trend analysis can be done using the function ‘vis.trends’ which estimates linear regressions for sliding periods of various lengths (Example 2.9, Figure 8). The results are presented visually with the strength of the trends shown as a colour scale on a grid where the x- and y-axes represent the starting point and the length of each period, respectively. Periods with statistically significant trends are marked with black outlines. The advantage of vis.trends is that it shows trends of various time scales, considering all variations of start- and end-points. The longest period is found in the upper left corner, representing the full length of the time series. The most recent period is shown in the bottom right corner. As demonstrated in Example 2.9, the strength and significance of estimated trends are sensitive to the period considered. The multiple period trend analysis is therefore a more robust alternative to single period trend fitting. ",
    "url": "http://localhost:4000/functions/#data-visualisation",
    "relUrl": "/functions/#data-visualisation"
  },"21": {
    "doc": "Welcome",
    "title": "Welcome",
    "content": "We present a tool, the R-package‘esd’, made freely available by the Norwegian MeteorologicalInstitute (MET Norway) for use by the climate community that: . | was primarily built for empirical-statistical downscaling of climate information and has been extended to search, process, dissect,and analyse meteorological and climatological data (localas well as global climate data sets). | consists of i) retrieving and manipulating large samples ofmeteorological, climate and model datafrom various sources, ii) searching, dissecting and displaying the information in the data, (iii) andcomputing a range of analyses. The acronym‘esd’can be associated with both ‘Easy &amp; SimpleData’ processing and ‘Empirical-Statistical downscaling’. | provides simple and intuitive ways forreading data from weather station, gridded data sets, as well as trajectory data such as cyclonepaths. | . The philosophy behind its design has been to reduce the time spent on coding, reformattingdata, testing the code, or looking up the manuals for correctsyntax. The functions are designed tobe intuitive and easy to remember in addition to being efficient. (PDF) Documentation for the climate analysis tool ‘esd’ (R-package). Available from: https://www.researchgate.net/publication/282348326Documentation_for_the_climate_analysis_tool‘esd’_R-package [accessed Dec 21 2020]. ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"22": {
    "doc": "Welcome",
    "title": "Support or Contact",
    "content": "Having trouble with ESD R package? Check out our documentation or contact support and we’ll help you sort it out. ",
    "url": "http://localhost:4000/#support-or-contact",
    "relUrl": "/#support-or-contact"
  },"23": {
    "doc": "Retrieve",
    "title": "Retrieving data: I/O",
    "content": "There are different types of data that can be handled by the ‘esd’ tool: station data, gridded data, and (storm) trajectories. Station data contain meteorological observations recorded at weather (or hydrological) stations, while gridded data can comprise various analyses (e.g. E-OBS gridded version of the European and Climate Assessment data set), reanalyses (e.g. NCEP, ERA, …) or global/regional climate model results (e.g. CMIP3/5 experiment). Trajectories are mainly used for analysis of storm tracks (e.g. IMILAST13). There are two main methods for retrieving data in the ‘esd’ tool: ‘station’ and ‘retrieve’. It is also possible to read data using the R-built in functions and convert it to esd data format. This requires more effort from the user using the ‘esd’ pre-defined functions as.station, as.field), and as.trajectory. The package comes with a set of sample data mainly included for demonstration purposes, testing, or troubleshooting. For gridded data, these are filtered data (mainly for reducing the size of the data objects) and should not be used as predictors in the actual analysis. For instance, air temperature reanalysis data are stored as a set of 20 empirical orthogonal functions (EOFs) with global coverage, which are then transformed to a field object upon a retrieval. However, the sample station data can be used without restrictions and corresponds to observed values at a specific location (e.g. data(Oslo) or nacd=station(src='nacd')). 11http://figshare.com/articles/esd_for_Mac_amp_Linux/1160493 12http://figshare.com/articles/esd_for_windows/1160494 13http://www.proclim.ch/imilast/index.html . ",
    "url": "http://localhost:4000/_pages/retrieve/#retrieving-data-io",
    "relUrl": "/_pages/retrieve/#retrieving-data-io"
  },"24": {
    "doc": "Retrieve",
    "title": "Function retrieve()",
    "content": "The ‘retrieve’ is designed to read data from NetCDF files following the standard Climate and Forecast (‘CF’) conventions and ordered on a longitude-latitude grid. The latter could be regular or irregular grid (e.g. the output of the Weather Research and Forecasting model (WRF) or any outputs on a rotated grid from RCMs). The function ‘retrieve’ also performs quick checks of the data itself to verify that the meta data and data are consistent. For instance, in case the frequency of the data is missing, retrieve() will try to detect the frequency automatically form the data itself. The function retrieve returns two types of objects depending on the type of the spatial grid. For instance, data stored on a regular grid is returned as field objects including attributes containing meta data, and data stored on irregular grid - such as rotated longitude-latitude grids - are returned as a station objects. The function retrieve() has also been adapted to read global climate data from the CMIP3/5 experiments, most of the global reanalysis such as those provided by the European Centre for Medium-Range Weather Forecasts (ECMWF) known as ERA-40 and ERA-INTERIM, the National Center for Environmental Prediction (NOAA) known as NCEP/NCAR reanalysis, the Japan Meteorological Agency (Japanese Reanalysis JRA-25,55), and the NASA GSFC Global Modeling and Assimilation Office (GMAO) known as MERRA reanalysis. A full overview of all available reanalysis can be found at http://reanalysis.org/atmosphere/overview-current-reanalyses. As for now, retrieve(), does not display a list of missing attributes that are mandatory for further post-processing of the data. The user must add the missing attributes manually. The strength of retrieve() is that it can read and return formatted objects with common attributes for post-processing, visualising and making outputs comparable (e.g. re-gridding the field objects into the same grid resolution). Basically, all reanalysis, general circulation models (GCMs), and regional climate models (RCMs) can be read using the ‘esd’ tool and further combined into one object, or analysed, albeit with some limitations due to the size of the returned object. ",
    "url": "http://localhost:4000/_pages/retrieve/#function-retrieve",
    "relUrl": "/_pages/retrieve/#function-retrieve"
  },"25": {
    "doc": "Retrieve",
    "title": "Function station()",
    "content": "The package ‘esd’ includes the function ‘station’ for obtaining historical climate data by querying various web portals, for instance, the MET Norway archive (KDVH) provided by the Norwegian Meteorological Institute14. Data from MET climate web service ‘eKlima’ needs to be adapted manually, the Global Historical Climate Network (GHCN, (Peterson and Vose,1997)) provided by the American National Climatic Data Center15, and the European Climate Assessment and Dataset (ECA&amp;D, (Klein Tank et al., 2002)) made available by the Royal Netherlands Meteorological Institute (KNMI) (http://eca.knmi.nl/). Some of the data that is 14http://eklima.met.no; however, this function only works within the firewall included in the package has been pre-formatted within the ‘clim.pact’ package and adapted to meet the new ‘esd’ data format requirements. 15http://www1.ncdc.noaa.gov/pub/ . SOURCE(S) : ECAD/GHCNM/METNOD/METNOM/NACD/NORDKLIM T2M/ 14632 2014 / 1773 ESD package − map.station() − MET Norway 2014 (www.met.no) Figure 1: Map of available weather stations recording temperature that are included in the meta-data of the ‘esd’ package. ",
    "url": "http://localhost:4000/_pages/retrieve/#function-station",
    "relUrl": "/_pages/retrieve/#function-station"
  },"26": {
    "doc": "Retrieve",
    "title": "Quick search - Function select.station()",
    "content": "The sample data includes also a meta-data object (stationmeta) that can be loaded directly and contains meta data such as name of the location (loc) and its standard ientification number (stid), geographical coordinates such as longitude (lon), latitude (lat), and altitude (alt), country name (country), parameter name (param) of recorded weather variables (e.g. temperature and precipitation), data source (src) or provider for thousands of stations all over the world (Figures 1 and 2). These meta-data have been imported from existing meta data from the different data source providers. It has to be noted also that most of the available stations are managed by the World Meteorological Organisation (WMO) and made available by the American National Climate Data Centre (NCDC) for scientific research only. The meta data has been merged from the different sources mentioned earlier. Also, other additional data sources can be easily included into the package. Figure 2: Map of available weather stations recording precipitation that are included in the meta-data of the ‘esd’ package. There are two ways of obtaining station data using station() method. The first option, which we recommend, is to select a subset of stations from the meta data using the function select.station() from a set of criteria. Among these criteria, the minimum length of the recorded data (‘nmin’) can be specified to get, for instance, a subset of stations recording for at least a minimum number of (e.g. 100) years of data (e.g. select.station(nmin=100)). Thus, a subsample of the meta data is returned and can be checked for duplications of stations from the various sources. Afterwards, users can start downloading the data for the selected stations. It has to be mentioned that the downloading process for the GHCN and ECA&amp;D is different as the data is stored differently. For the GHCN data sets, each station is stored separately and a direct retrieval can be made. We highly recommend that users perform a prior selection of stations for the GHCN data sets before starting the downloading process. For the ECA&amp;D data sets on the other hand, all stations are downloaded and stored locally as zip files at first call, after which data is extracted for the selection of stations. Other data sets, such as the NACD (Frich et al., 1996), NARP (Førland ), and Nordklim (Tuomenvirta et al., 2001) are stored entirely in ‘esd’ (only holds monthly values of a limited set). The ‘station’ method can retrieve station data from a range of different sources, many over the web (GHCN and ECA&amp;D). Example 2.1 demonstrates how to select, retrieve and plot temperature observations from a single station. The ‘esd’ tool holds meta-data for various data sources, which makes it quick and easy to search for weather data recorded at stations according to the parameter, the geographical location (region, country, and coordinates (single or a range of longitude and latitude values) and altitude, and time interval. ",
    "url": "http://localhost:4000/_pages/retrieve/#quick-search---function-selectstation",
    "relUrl": "/_pages/retrieve/#quick-search---function-selectstation"
  },"27": {
    "doc": "Retrieve",
    "title": "Retrieve",
    "content": " ",
    "url": "http://localhost:4000/_pages/retrieve/",
    "relUrl": "/_pages/retrieve/"
  }
}
